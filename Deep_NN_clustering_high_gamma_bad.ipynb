{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep-NN_clustering_high_gamma_bad.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mxZOMIDXXau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import math\n",
        "import sys\n",
        "import logging\n",
        "#-----------------------------------------------------------\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from IPython.display import clear_output\n",
        "from scipy.spatial.distance import squareform, pdist\n",
        "from sklearn.preprocessing import normalize\n",
        "from numpy import linalg as LA\n",
        "from scipy.cluster.vq import kmeans, vq\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "from math import sqrt\n",
        "#----------------------------------------------\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from optparse import OptionParser\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYqX8ZR5wsGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def standardization(X):\n",
        "    return normalize(X, axis=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKho_sESzQMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def laplacian(A):\n",
        "    S = np.sum(A, 0)\n",
        "    D = np.diag(S)\n",
        "    D = LA.matrix_power(D, -1)\n",
        "    L = np.dot(D, A)\n",
        "    return L"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pUSpsQozS-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization(V):\n",
        "    return (V - min(V)) / (max(V) - min(V))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hndbmGyJzWIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Correlation_Similarity:\n",
        "    def get_matrix(self, Data):\n",
        "        X = standardization(Data)\n",
        "        X = pdist(X, 'correlation')\n",
        "        X = squareform(X)\n",
        "        L = laplacian(X)\n",
        "        Y = np.apply_along_axis(normalization, 1, L)\n",
        "        return Y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHrGqwRnzY-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Cosine_Similarity:\n",
        "    def get_matrix(self, Data):\n",
        "        X = standardization(Data)\n",
        "        X = pdist(X, 'cosine')\n",
        "        X = squareform(X)\n",
        "        L = laplacian(X)\n",
        "        Y = np.apply_along_axis(normalization, 1, L)\n",
        "        return Y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRV0iHGWzcp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Similarity_Dataset_Iterator():\n",
        "    def __init__(self, data, labels, similarity):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.matrix = similarity.get_matrix(data)\n",
        "        self.data_size = self.matrix.shape[0]\n",
        "        self.current_index = 0\n",
        "    def next_batch(self, num):\n",
        "        data=self.matrix.transpose()\n",
        "        labels=self.labels\n",
        "        idx = np.arange(0 , len(data))\n",
        "        np.random.shuffle(idx)\n",
        "        idx = idx[:num]\n",
        "        data_shuffle = [data[ i] for i in idx]\n",
        "        labels_shuffle = [labels[ i] for i in idx]\n",
        "        return data_shuffle, labels_shuffle\n",
        "    def whole_dataset(self):\n",
        "        return (self.matrix.transpose(), self.labels)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n076atQh0kmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3U9x9aT05Qz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "44ddfbbb-b183-45cb-9b67-b8d0f56f9bf6"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/edited_topics_set2.csv')\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>_id</th>\n",
              "      <th>topic</th>\n",
              "      <th>content</th>\n",
              "      <th>processed_content</th>\n",
              "      <th>processed_topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5f04e496ef217aae6a201f71</td>\n",
              "      <td>[\"National\"]</td>\n",
              "      <td>[\"The West Bengal government on Tuesday decide...</td>\n",
              "      <td>west bengal govern tuesday decid impos complet...</td>\n",
              "      <td>nation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5f04e498ef217aae6a201f72</td>\n",
              "      <td>[\"Business\"]</td>\n",
              "      <td>[\"The government is weighing the pros and cons...</td>\n",
              "      <td>govern weigh pros con halt import includ china...</td>\n",
              "      <td>busi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5f04e49aef217aae6a201f73</td>\n",
              "      <td>[\"National\"]</td>\n",
              "      <td>[\"The Central Board of Secondary Education (CB...</td>\n",
              "      <td>central board secondari educ cbse slash syllab...</td>\n",
              "      <td>nation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>5f04e49def217aae6a201f74</td>\n",
              "      <td>[\"International\"]</td>\n",
              "      <td>[\"The World Health Organization on Tuesday ack...</td>\n",
              "      <td>world health organ tuesday acknowledg emerg ev...</td>\n",
              "      <td>intern</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5f04e49fef217aae6a201f75</td>\n",
              "      <td>[\"International\"]</td>\n",
              "      <td>[\"President Donald Trump on Tuesday formally s...</td>\n",
              "      <td>presid donald trump tuesday formal start withd...</td>\n",
              "      <td>intern</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...  processed_topic\n",
              "0           0  ...           nation\n",
              "1           1  ...             busi\n",
              "2           2  ...           nation\n",
              "3           3  ...           intern\n",
              "4           4  ...           intern\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0GlS3WC1Udh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.dropna(subset=['content'], inplace = True)\n",
        "#df.dropna(subset=['topic'], inplace = True)\n",
        "#df['your column name'].isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dat6uH3E2OmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#len(df.content.unique()),  len(df.content), len(df.topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S87soJgE2SZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PbPTbyu2deC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2941fd92-4f9f-4570-ae64-be792e00dc36"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2155, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0qrfJGtSkVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebb68e5f-1e6d-40cb-904e-38a0f025f269"
      },
      "source": [
        "np.unique(df.processed_topic[:600]).shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-XKjCAS48pD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import re\n",
        "\n",
        "#import gensim\n",
        "#from gensim import corpora,models\n",
        "#from gensim.utils import simple_preprocess\n",
        "#from gensim.models import CoherenceModel\n",
        "#from gensim.parsing.preprocessing import STOPWORDS\n",
        "#from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "#from nltk.stem.porter import *\n",
        "#import numpy as np\n",
        "#np.random.seed(2018)\n",
        "\n",
        "#import nltk\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2dezj2a48qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#stemmer = SnowballStemmer(language='english',ignore_stopwords=True)\n",
        "#def lemmatize_stemming(text):\n",
        "#    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "#def preprocess(text):\n",
        "    #result = []\n",
        "   # for token in gensim.utils.simple_preprocess(text):\n",
        "   #     if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "   #         result.append(lemmatize_stemming(token))\n",
        "  #  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws57P9WE48uI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#doc_sample = df['content'][0]\n",
        "#print('original document: ')\n",
        "#words = []\n",
        "#for word in doc_sample.split(' '):\n",
        "#    words.append(word)\n",
        "#print(words)\n",
        "#print('\\n\\n tokenized and lemmatized document: ')\n",
        "#print(preprocess(doc_sample))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzga2tYu48mP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df['processed_heading'] = df['heading'].map(preprocess)\n",
        "#df['processed_content'] = df['content'].map(preprocess)\n",
        "#df['processed_topic'] = df['topic'].map(preprocess)\n",
        "#df['processed_heading'] = df['processed_heading'].apply(lambda x: ' '.join(x))\n",
        "#df['processed_content'] = df['processed_content'].apply(lambda x: ' '.join(x))\n",
        "#df['processed_topic'] = df['processed_topic'].apply(lambda x: ' '.join(x))\n",
        "#df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfAJav1NLOVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('/content/drive/My Drive/edited_all_news.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsa-O7Bw48kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from collections import Counter\n",
        "#Counter(df.topic[700:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQBuLrsC3FWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_NewsGroup_data(similarity):    \n",
        "    logging.basicConfig(level=logging.INFO,\n",
        "                        format='%(asctime)s %(levelname)s %(message)s')\n",
        "    op = OptionParser()\n",
        "    op.add_option(\"--lsa\", dest=\"n_components\", type=\"int\",\n",
        "                  help=\"Preprocess documents with latent semantic analysis.\")    \n",
        "    op.add_option(\"--no-idf\",action=\"store_false\", dest=\"use_idf\", default=True,\n",
        "                  help=\"Disable Inverse Document Frequency feature weighting.\")\n",
        "    op.add_option(\"--use-hashing\", action=\"store_true\", default=False,\n",
        "                  help=\"Use a hashing feature vectorizer\")\n",
        "    op.add_option(\"--n-features\", type=int, default=10000,\n",
        "                  help=\"Maximum number of features to extract from text.\")    \n",
        "    def is_interactive():\n",
        "        return not hasattr(sys.modules['__main__'], '__file__')\n",
        "    argv = [] if is_interactive() else sys.argv[1:]\n",
        "    (opts, args) = op.parse_args(argv)\n",
        "    if len(args) > 0:\n",
        "        op.error(\"this script takes no arguments.\")\n",
        "        sys.exit(1)\n",
        "        \n",
        "    labels = df.processed_topic\n",
        "    data = df.processed_content\n",
        "    #true_k = np.unique(labels).shape[0]\n",
        "    vectorizer = TfidfVectorizer(max_features=opts.n_features,use_idf=opts.use_idf)\n",
        "    X = vectorizer.fit_transform(data)\n",
        "    if opts.n_components:\n",
        "        svd = TruncatedSVD(opts.n_components)\n",
        "        normalizer = Normalizer(copy=False)\n",
        "        lsa = make_pipeline(svd, normalizer)\n",
        "        X = lsa.fit_transform(X)\n",
        "        explained_variance = svd.explained_variance_ratio_.sum()\n",
        "    return Similarity_Dataset_Iterator(X.toarray(), labels, similarity)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZHYkqiV6Gsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call Correlation_Similarity as similarity dataset.\n",
        "trainSet_correlation = read_NewsGroup_data(Correlation_Similarity())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZQ9wy646MCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call Cosine_Similarity as similarity dataset.\n",
        "trainSet_cosine = read_NewsGroup_data(Cosine_Similarity())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP5z_PXlZMB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZQCnGSnZL-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "0e54a66e-e872-4697-d49a-40c6c3815f40"
      },
      "source": [
        "n_input = trainSet_correlation.data_size #--------- Number of input data.\n",
        "# Define the number of hidden layer. \n",
        "if n_input >= 1024:\n",
        "    Nn = int(2048)\n",
        "elif n_input >= 512:\n",
        "    Nn = int(1024)\n",
        "elif n_input >= 256:\n",
        "    Nn = int(512)\n",
        "\n",
        "n_hidden_1 = int(Nn/2) #-------------------- The autoencoder hidden layer 1.\n",
        "n_code = str(int(n_hidden_1/2)) #----------- The number of output dimension value.\n",
        "\n",
        "print('Layer 1: -----------', n_input)\n",
        "print('Layer 2: -----------', n_hidden_1)\n",
        "print('Layer 3: -----------', int(n_code))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer 1: ----------- 2155\n",
            "Layer 2: ----------- 1024\n",
            "Layer 3: ----------- 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0McRy7WF6gX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_means_(X, n_clusters):\n",
        "    kmeans_centroids,_ =  kmeans(X, n_clusters)\n",
        "    kmeans_, _ = vq(X, kmeans_centroids)\n",
        "    return kmeans_"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWRwNRZk9nij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder(x, n_code, phase_train):    \n",
        "    with tf.variable_scope(\"encoder\"):        \n",
        "        with tf.variable_scope(\"hidden-layer-1\"):\n",
        "            hidden_1 = layer(x, [n_input, n_hidden_1], [n_hidden_1], mode_train)\n",
        "        with tf.variable_scope(\"hidden-layer-2\"):\n",
        "            hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2], mode_train)\n",
        "        with tf.variable_scope(\"hidden-layer-3\"):\n",
        "            hidden_3 = layer(hidden_2, [n_hidden_2, n_hidden_3], [n_hidden_3], mode_train)        \n",
        "        with tf.variable_scope(\"code\"):\n",
        "            code = layer(hidden_3, [n_hidden_3, n_code], [n_code], mode_train)\n",
        "    return code\n",
        "\n",
        "def decoder(code, n_code, mode_train):\n",
        "    with tf.variable_scope(\"decoder\"):\n",
        "        with tf.variable_scope(\"hidden-layer-1\"):\n",
        "            hidden_1 = layer(code, [n_code, n_hidden_3], [n_hidden_3], mode_train)\n",
        "        with tf.variable_scope(\"hidden-layer-2\"):\n",
        "            hidden_2 = layer(hidden_1, [n_hidden_3, n_hidden_2], [n_hidden_2], mode_train)\n",
        "        with tf.variable_scope(\"hidden-layer-3\"):\n",
        "            hidden_3 = layer(hidden_2, [n_hidden_2, n_hidden_1], [n_hidden_1], mode_train)              \n",
        "        with tf.variable_scope(\"reconstructed\"):\n",
        "            output = layer(hidden_3, [n_hidden_1, n_input], [n_input], mode_train)\n",
        "    return output"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu5anCDN2-4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_norm(x, n_out, mode_train):\n",
        "    beta_initialize = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
        "    gamma_initialize = tf.constant_initializer(value=1.0, dtype=tf.float32)\n",
        "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_initialize)\n",
        "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_initialize)\n",
        "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
        "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
        "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
        "    def mean_var():\n",
        "        with tf.control_dependencies([ema_apply_op]):\n",
        "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "    mean, var = control_flow_ops.cond(mode_train, mean_var, lambda: (ema_mean, ema_var))\n",
        "    reshaped_x = tf.reshape(x, [-1, 1, 1, n_out])\n",
        "    normed = tf.nn.batch_norm_with_global_normalization(reshaped_x, mean, var, beta, gamma, 1e-3, True)\n",
        "    return tf.reshape(normed, [-1, n_out])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0mjUhRR9uJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layer(input, weight_shape, bias_shape, mode_train):\n",
        "    value_initialize = (1.0 / weight_shape[0] ** 0.5)\n",
        "    weight_initialize = tf.random_normal_initializer(stddev = value_initialize, seed = None)\n",
        "    bias_initialize = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
        "    w = tf.get_variable(\"w\", weight_shape, initializer=weight_initialize)\n",
        "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_initialize)\n",
        "    return tf.nn.sigmoid(batch_norm((tf.matmul(input, w) + b), weight_shape[1], mode_train))\n",
        "\n",
        "def loss(reconstructed, x):\n",
        "    with tf.variable_scope(\"train\"):\n",
        "        train_loss = tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(reconstructed, x)), 1))\n",
        "        return train_loss\n",
        "\n",
        "def training(cost, learning_rate, beta1, beta2, global_step):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon=1e-08, use_locking=False, name='Adam')\n",
        "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
        "    return train_op"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAy3evZM94SW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "n_layers = 3 #----------------------------- Number of Neural Networks Layers.\n",
        "beta1 = 0.9 #------------------------------ The decay rate 1.  \n",
        "beta2 = 0.999 #---------------------------- The decay rate 2.\n",
        "learning_rate = (beta1/n_input) #---------- The learning rate.\n",
        "stop_learning = 1.35 #--------------------- The stop learning point.\n",
        "#n_batch = math.ceil(sqrt(sqrt(n_input))) #- Number of selection data in per step.\n",
        "n_batch = 32\n",
        "n_backpro = math.ceil(n_input/n_batch) #--- Number of Backpro in per epoch.\n",
        "n_clusters = 3 #--------------------------- Number of clusters.\n",
        "n_diplay = 10 #---------------------------- Number of getting code and runnig the K-Means."
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBe921XG3QeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_cor, labels_cor = trainSet_correlation.whole_dataset() #-- Allocation of data and labels\n",
        "data_cos, labels_cos = trainSet_cosine.whole_dataset() #------- Allocation of data and labels\n",
        "\n",
        "results_cor=[] #--------------------------- A list to keep all NMI scores.\n",
        "loss_cost_cor=[] #------------------------- A list to keep all training evaluations.\n",
        "steps_cor=[] #----------------------------- A list to keep all steps."
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD5MgDGl_n1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seeding_cor = []"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktiwp6Ko-E77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e47091b9-1c5a-4068-cff8-f4c89c09a9f2"
      },
      "source": [
        "print(n_batch)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftu5VPVKfXqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Graph().as_default():    \n",
        "    with tf.variable_scope(\"autoencoder_architecture\"):\n",
        "        x = tf.placeholder(\"float\", [None, n_input])   \n",
        "        mode_train = tf.placeholder(tf.bool)\n",
        "        code = encoder(x, int(n_code), mode_train)\n",
        "        reconstructed = decoder(code, int(n_code), mode_train)\n",
        "        cost = loss(reconstructed, x)\n",
        "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        train_optimizer = training(cost, learning_rate, beta1, beta2, global_step)\n",
        "        sess = tf.Session()\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        sess.run(init_op)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzhSKWs97k7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "059a1be6-3de6-45b0-e6eb-0e1a09b67d38"
      },
      "source": [
        "# Training cycle\n",
        "epoch = 0\n",
        "while epoch == 0 or new_cost >= stop_learning:\n",
        "    # Fit training with Backpropagation using batch data.\n",
        "    for i in range(n_backpro):\n",
        "        miniData, _ = trainSet_correlation.next_batch(n_batch)\n",
        "        _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n",
        "                                                                  mode_train:True})       \n",
        "    #------------------------- End of the Optimization ------------------------------\n",
        "    epoch += 1\n",
        "    # Save the results after per 10 epochs.    \n",
        "    if epoch % n_diplay == 0 or new_cost <= stop_learning:\n",
        "        # Getting embedded codes and running K-Means on them.\n",
        "        ae_codes_cor = sess.run(code, feed_dict={x: data_cor, mode_train: False})        \n",
        "        idx_cor = k_means_(ae_codes_cor, n_clusters)\n",
        "        ae_nmi_cor = normalized_mutual_info_score(labels_cor, idx_cor)\n",
        "        ae_nmi_cor = ae_nmi_cor*100\n",
        "        results_cor.append(ae_nmi_cor)    \n",
        "        steps_cor.append(epoch)\n",
        "        loss_cost_cor.append(new_cost)    \n",
        "        print(\"NMI Score for AE is: {:0.2f} and new cost is: {:0.2f} in {:d} step. \"\n",
        "              .format(ae_nmi_cor, new_cost, epoch))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NMI Score for AE is: 14.49 and new cost is: 422.72 in 10 step. \n",
            "NMI Score for AE is: 11.32 and new cost is: 329.58 in 20 step. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-93627f484815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mminiData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainSet_correlation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n\u001b[0;32m----> 8\u001b[0;31m                                                                   mode_train:True})       \n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#------------------------- End of the Optimization ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ulspmFn7tu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The Average of NMI Score for >>> {:d} <<< Random Factors in Autoencoder Correlation is >>> {:0.2f} <<<\"\n",
        "      .format(20, (np.mean(results_cor))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND1MKJ6iE8Ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_cor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmhBQfyM3nVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_cos=[] #--------------------------- A list to keep all NMI scores.\n",
        "loss_cost_cos=[] #------------------------- A list to keep all training evaluations.\n",
        "steps_cos=[] #----------------------------- A list to keep all steps."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRlIuAoYAAnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Graph().as_default():    \n",
        "    with tf.variable_scope(\"autoencoder_architecture\"):\n",
        "        x = tf.placeholder(\"float\", [None, n_input])   \n",
        "        mode_train = tf.placeholder(tf.bool)\n",
        "        code = encoder(x, int(n_code), mode_train)\n",
        "        reconstructed = decoder(code, int(n_code), mode_train)\n",
        "        cost = loss(reconstructed, x)\n",
        "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "        train_optimizer = training(cost, learning_rate, beta1, beta2, global_step)\n",
        "        sess = tf.Session()\n",
        "        init_op = tf.global_variables_initializer()\n",
        "        sess.run(init_op)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TERMUm14E8SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch = 0\n",
        "while epoch == 0 or new_cost >= stop_learning:\n",
        "    # Fit training with backpropagation using batch data.\n",
        "    for i in range(n_backpro):\n",
        "        miniData, _ = trainSet_cosine.next_batch(n_batch)\n",
        "        _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n",
        "                                                                  mode_train: True})       \n",
        "    #------------------------- End of the Optimization ------------------------------\n",
        "    epoch += 1\n",
        "    # Save the results after per 10 epochs.    \n",
        "    if epoch % n_diplay == 0 or new_cost <= stop_learning:\n",
        "        # Getting embedded codes and running K-Means on them.\n",
        "        ae_codes_cos = sess.run(code, feed_dict={x: data_cos, mode_train: False})        \n",
        "        idx_cos = k_means_(ae_codes_cos, n_clusters)\n",
        "        ae_nmi_cos = normalized_mutual_info_score(labels_cos, idx_cos)\n",
        "        ae_nmi_cos = ae_nmi_cos*100\n",
        "        results_cos.append(ae_nmi_cos)    \n",
        "        steps_cos.append(epoch)\n",
        "        loss_cost_cos.append(new_cost)    \n",
        "        print(\"NMI Score for AE is: {:0.2f} and new cost is: {:0.2f} in {:d} step. \"\n",
        "              .format(ae_nmi_cos, new_cost, epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL6sjMQEFIXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The Average of NMI Score for >>> {:d} <<< Random Factors in Autoencoder Cosine is >>> {:0.2f} <<<\"\n",
        "      .format(len(seeding_cos), (np.mean(results_cos))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRQmt5pjFIa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_cos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3afCnSOFIVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "plt.figure(figsize=(12,3.5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(steps_cor, loss_cost_cor, label='Cost Trianing for Correlation Distance ', color='#000080', marker='o')\n",
        "plt.plot(steps_cos, loss_cost_cos, label='Cost Trianing for Cosine Distance ', color='#E3CF57', marker='s')\n",
        "plt.xlabel('Number of Epochs.')\n",
        "plt.ylabel('Cost')\n",
        "plt.grid()\n",
        "plt.title('Cost Function Trianing')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(1,2,2)\n",
        "plt.ylim(1, 90)\n",
        "plt.plot(steps_cor, results_cor, label='AE Normalized Correlation Distance ', color='#000080', marker='o')\n",
        "plt.plot(steps_cos, results_cos, label='AE Normalized Cosine Distance ', color='#E3CF57', marker='s')\n",
        "plt.xlabel('Number of Epochs.')\n",
        "plt.ylabel('NMI')\n",
        "plt.grid()\n",
        "plt.title(('NMI of AE Correlation is {:0.2f} \\n and AE Cosine is {:0.2f}').format(ae_nmi_cor, ae_nmi_cos))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHeH536bYLm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "origin_label_cos = np.array(trainSet_cosine.whole_dataset()[1]).astype(int)\n",
        "origin_label_cor = np.array(trainSet_correlation.whole_dataset()[1]).astype(int)\n",
        "colors = [('c', '1'),('g', '2'),('m','3')]\n",
        "plt.figure(figsize=(14, 5))\n",
        "for num in range(3):\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.scatter([ae_codes_cor[:,0][i] for i in range(len(origin_label_cor)) if origin_label_cor[i] == num],\n",
        "                [ae_codes_cor[:,1][i] for i in range(len(origin_label_cor)) if origin_label_cor[i] == num],\n",
        "                60, label=str(num+1), color = colors[num][0], marker=colors[num][1])\n",
        "    plt.title('Normalized Correlation Distance with Original Labels.')\n",
        "    plt.xlabel('A representation by AE in 2 dimensions.')\n",
        "    plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.scatter([ae_codes_cos[:,0][i] for i in range(len(origin_label_cos)) if origin_label_cos[i] == num],\n",
        "                [ae_codes_cos[:,1][i] for i in range(len(origin_label_cos)) if origin_label_cos[i] == num],\n",
        "                60, label=str(num+1), color = colors[num][0], marker=colors[num][1])\n",
        "    plt.title('Normalized Cosine Distance with Original Labels.')\n",
        "    plt.xlabel('A representation by AE in 2 dimensions.')\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP8f1rkvY16D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "colors = [('r', '1'),('b', '2'),('y','3')]\n",
        "plt.figure(figsize=(14, 5))\n",
        "for num in range(3):\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.scatter([ae_codes_cor[:,0][i] for i in range(len(idx_cor)) if idx_cor[i] == num],\n",
        "                [ae_codes_cor[:,1][i] for i in range(len(idx_cor)) if idx_cor[i] == num],\n",
        "                60, label=str(num+1), color = colors[num][0], marker=colors[num][1])\n",
        "    plt.title(('NMI of AE on Correlation is {:0.2f}').format(ae_nmi_cor))\n",
        "    plt.xlabel('Runs K-Means on the representation by AE in 2 dimensions.')\n",
        "    plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.scatter([ae_codes_cos[:,0][i] for i in range(len(idx_cos)) if idx_cos[i] == num],\n",
        "                [ae_codes_cos[:,1][i] for i in range(len(idx_cos)) if idx_cos[i] == num],\n",
        "                60, label=str(num+1), color = colors[num][0], marker=colors[num][1])\n",
        "    plt.title(('NMI of AE on Cosine is {:0.2f}').format(ae_nmi_cos))\n",
        "    plt.xlabel('Runs K-Means on the representation by AE in 2 dimensions.')\n",
        "    plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2MIUHItepur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Autoencoder Clustering on Cosine: ------------ {:0.2f}\".format(ae_nmi_cos))\n",
        "print(\"Autoencoder Clustering on Correlation: ------- {:0.2f}\".format(ae_nmi_cor))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB-_VmUue99g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}