{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_subspace_Clustering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdTMQVyExD4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.contrib import layers\n",
        "from sklearn import cluster\n",
        "from munkres import Munkres\n",
        "import scipy.io as sio\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.preprocessing import normalize\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faIH0QbpxN3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvAE(object):\n",
        "\tdef __init__(self, n_input, kernel_size, n_hidden, reg_const1 = 1.0, reg_const2 = 1.0, reg = None, batch_size = 256,\\\n",
        "\t\tdenoise = False, model_path = None, logs_path = '/content/drive/My Drive/deep-subspace-clustering/COIL20CodeModel/new/pretrain/logs'):\t\n",
        "\t#n_hidden is a arrary contains the number of neurals on every layer\n",
        "\t\tself.n_input = n_input\n",
        "\t\tself.n_hidden = n_hidden\n",
        "\t\tself.reg = reg\n",
        "\t\tself.model_path = model_path\t\t\n",
        "\t\tself.kernel_size = kernel_size\t\t\n",
        "\t\tself.iter = 0\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tweights = self._initialize_weights()\n",
        "\t\t\n",
        "\t\t# model\n",
        "\t\tself.x = tf.placeholder(tf.float32, [None, self.n_input[0], self.n_input[1], 1])\n",
        "\t\tself.learning_rate = tf.placeholder(tf.float32, [])\n",
        "\t\t\n",
        "\t\tif denoise == False:\n",
        "\t\t\tx_input = self.x\n",
        "\t\t\tlatent, shape = self.encoder(x_input, weights)\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tx_input = tf.add(self.x, tf.random_normal(shape=tf.shape(self.x),\n",
        "\t\t\t\t\t\t\t\t\t\t\t   mean = 0,\n",
        "\t\t\t\t\t\t\t\t\t\t\t   stddev = 0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t   dtype=tf.float32))\n",
        "\n",
        "\t\t\tlatent,shape = self.encoder(x_input, weights)\n",
        "\t\tself.z_conv = tf.reshape(latent,[batch_size, -1])\t\t\n",
        "\t\tself.z_ssc, Coef = self.selfexpressive_moduel(batch_size)\t\n",
        "\t\tself.Coef = Coef\t\t\t\t\t\t\n",
        "\t\tlatent_de_ft = tf.reshape(self.z_ssc, tf.shape(latent))\t\t\n",
        "\t\tself.x_r_ft = self.decoder(latent_de_ft, weights, shape)\t\t\n",
        "\t\t\t\t\n",
        "\n",
        "\t\tself.saver = tf.train.Saver([v for v in tf.trainable_variables() if not (v.name.startswith(\"Coef\"))]) \n",
        "\t\t\t\n",
        "\t\t\n",
        "\t\tself.cost_ssc = 0.5*tf.reduce_sum(tf.pow(tf.subtract(self.z_conv,self.z_ssc), 2))\n",
        "\t\tself.recon_ssc =  tf.reduce_sum(tf.pow(tf.subtract(self.x_r_ft, self.x), 2.0))\n",
        "\t\tself.reg_ssc = tf.reduce_sum(tf.pow(self.Coef,2))\n",
        "\t\ttf.summary.scalar(\"ssc_loss\", self.cost_ssc)\n",
        "\t\ttf.summary.scalar(\"reg_lose\", self.reg_ssc)\t\t\n",
        "\t\t\n",
        "\t\tself.loss_ssc = self.cost_ssc*reg_const2 + reg_const1*self.reg_ssc + self.recon_ssc\n",
        "\n",
        "\t\tself.merged_summary_op = tf.summary.merge_all()\t\t\n",
        "\t\tself.optimizer_ssc = tf.train.AdamOptimizer(learning_rate = self.learning_rate).minimize(self.loss_ssc)\n",
        "\t\tself.init = tf.global_variables_initializer()\n",
        "\t\tself.sess = tf.InteractiveSession()\n",
        "\t\tself.sess.run(self.init)\n",
        "\t\tself.summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
        "\n",
        "\tdef _initialize_weights(self):\n",
        "\t\tall_weights = dict()\n",
        "\t\tall_weights['enc_w0'] = tf.get_variable(\"enc_w0\", shape=[self.kernel_size[0], self.kernel_size[0], 1, n_hidden[0]],\n",
        "\t\t\tinitializer=layers.xavier_initializer_conv2d(),regularizer = self.reg)\n",
        "\t\tall_weights['enc_b0'] = tf.Variable(tf.zeros([self.n_hidden[0]], dtype = tf.float32))\n",
        "\t\t\n",
        "\n",
        "\t\tall_weights['dec_w0'] = tf.get_variable(\"dec_w0\", shape=[self.kernel_size[0], self.kernel_size[0],1, n_hidden[0]],\n",
        "\t\t\tinitializer=layers.xavier_initializer_conv2d(),regularizer = self.reg)\n",
        "\t\tall_weights['dec_b0'] = tf.Variable(tf.zeros([1], dtype = tf.float32))\n",
        "\t\treturn all_weights\n",
        "\n",
        "\n",
        "\t# Building the encoder\n",
        "\tdef encoder(self,x, weights):\n",
        "\t\tshapes = []\n",
        "\t\t# Encoder Hidden layer with relu activation #1\n",
        "\t\tshapes.append(x.get_shape().as_list())\n",
        "\t\tlayer1 = tf.nn.bias_add(tf.nn.conv2d(x, weights['enc_w0'], strides=[1,2,2,1],padding='SAME'),weights['enc_b0'])\n",
        "\t\tlayer1 = tf.nn.relu(layer1)\n",
        "\t\treturn  layer1, shapes\n",
        "\n",
        "\t# Building the decoder\n",
        "\tdef decoder(self,z, weights, shapes):\n",
        "\t\t# Encoder Hidden layer with relu activation #1\n",
        "\t\tshape_de1 = shapes[0]\n",
        "\t\tlayer1 = tf.add(tf.nn.conv2d_transpose(z, weights['dec_w0'], tf.stack([tf.shape(self.x)[0],shape_de1[1],shape_de1[2],shape_de1[3]]),\\\n",
        "\t\t strides=[1,2,2,1],padding='SAME'),weights['dec_b0'])\n",
        "\t\tlayer1 = tf.nn.relu(layer1)\n",
        "\t\t\n",
        "\t\treturn layer1\n",
        "\n",
        "\n",
        "\n",
        "\tdef selfexpressive_moduel(self,batch_size):\n",
        "\t\t\n",
        "\t\tCoef = tf.Variable(1.0e-8 * tf.ones([self.batch_size, self.batch_size],tf.float32), name = 'Coef')\t\t\t\n",
        "\t\tz_ssc = tf.matmul(Coef,\tself.z_conv)\n",
        "\t\treturn z_ssc, Coef\n",
        "\n",
        "\n",
        "\tdef finetune_fit(self, X, lr):\n",
        "\t\tC,l1_cost, l2_cost, summary, _ = self.sess.run((self.Coef, self.reg_ssc, self.cost_ssc, self.merged_summary_op, self.optimizer_ssc), \\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tfeed_dict = {self.x: X, self.learning_rate: lr})\n",
        "\t\tself.summary_writer.add_summary(summary, self.iter)\n",
        "\t\tself.iter = self.iter + 1\n",
        "\t\treturn C, l1_cost,l2_cost \n",
        "\t\n",
        "\tdef initlization(self):\n",
        "\t\ttf.reset_default_graph()\n",
        "\t\tself.sess.run(self.init)\t\n",
        "\n",
        "\tdef transform(self, X):\n",
        "\t\treturn self.sess.run(self.z_conv, feed_dict = {self.x:X})\n",
        "\n",
        "\tdef save_model(self):\n",
        "\t\tsave_path = self.saver.save(self.sess,self.model_path)\n",
        "\t\tprint (\"model saved in file: %s\" % save_path)\n",
        "\n",
        "\tdef restore(self):\n",
        "\t\tself.saver.restore(self.sess, self.model_path)\n",
        "\t\tprint (\"model restored\")\n",
        "\t\t\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuaSiOA5xipM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def best_map(L1,L2):\n",
        "\t#L1 should be the labels and L2 should be the clustering number we got\n",
        "\tLabel1 = np.unique(L1)\n",
        "\tnClass1 = len(Label1)\n",
        "\tLabel2 = np.unique(L2)\n",
        "\tnClass2 = len(Label2)\n",
        "\tnClass = np.maximum(nClass1,nClass2)\n",
        "\tG = np.zeros((nClass,nClass))\n",
        "\tfor i in range(nClass1):\n",
        "\t\tind_cla1 = L1 == Label1[i]\n",
        "\t\tind_cla1 = ind_cla1.astype(float)\n",
        "\t\tfor j in range(nClass2):\n",
        "\t\t\tind_cla2 = L2 == Label2[j]\n",
        "\t\t\tind_cla2 = ind_cla2.astype(float)\n",
        "\t\t\tG[i,j] = np.sum(ind_cla2 * ind_cla1)\n",
        "\tm = Munkres()\n",
        "\tindex = m.compute(-G.T)\n",
        "\tindex = np.array(index)\n",
        "\tc = index[:,1]\n",
        "\tnewL2 = np.zeros(L2.shape)\n",
        "\tfor i in range(nClass2):\n",
        "\t\tnewL2[L2 == Label2[i]] = Label1[c[i]]\n",
        "\treturn newL2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v50tghVOxk-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def thrC(C,ro):\n",
        "\tif ro < 1:\n",
        "\t\tN = C.shape[1]\n",
        "\t\tCp = np.zeros((N,N))\n",
        "\t\tS = np.abs(np.sort(-np.abs(C),axis=0))\n",
        "\t\tInd = np.argsort(-np.abs(C),axis=0)\n",
        "\t\tfor i in range(N):\n",
        "\t\t\tcL1 = np.sum(S[:,i]).astype(float)\n",
        "\t\t\tstop = False\n",
        "\t\t\tcsum = 0\n",
        "\t\t\tt = 0\n",
        "\t\t\twhile(stop == False):\n",
        "\t\t\t\tcsum = csum + S[t,i]\n",
        "\t\t\t\tif csum > ro*cL1:\n",
        "\t\t\t\t\tstop = True\n",
        "\t\t\t\t\tCp[Ind[0:t+1,i],i] = C[Ind[0:t+1,i],i]\n",
        "\t\t\t\tt = t + 1\n",
        "\telse:\n",
        "\t\tCp = C\n",
        "\n",
        "\treturn Cp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNXBABWAxneS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def post_proC(C, K, d, alpha):\n",
        "\t# C: coefficient matrix, K: number of clusters, d: dimension of each subspace\n",
        "\tn = C.shape[0]\n",
        "\tC = 0.5*(C + C.T)\t \n",
        "\tC = C - np.diag(np.diag(C)) + np.eye(n,n) # for sparse C, this step will make the algorithm more numerically stable\n",
        "\tr = d*K + 1\t\t\n",
        "\tU, S, _ = svds(C,r,v0 = np.ones(n))\n",
        "\tU = U[:,::-1] \n",
        "\tS = np.sqrt(S[::-1])\n",
        "\tS = np.diag(S)\n",
        "\tU = U.dot(S)\n",
        "\tU = normalize(U, norm='l2', axis = 1)  \n",
        "\tZ = U.dot(U.T)\n",
        "\tZ = Z * (Z>0)\n",
        "\tL = np.abs(Z ** alpha)\n",
        "\tL = L/L.max()\n",
        "\tL = 0.5 * (L + L.T)\t\n",
        "\tspectral = cluster.SpectralClustering(n_clusters=K, eigen_solver='arpack', affinity='precomputed', assign_labels='discretize')\n",
        "\tspectral.fit(L)\n",
        "\tgrp = spectral.fit_predict(L) + 1\n",
        "\treturn grp, L\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWXN1RQlxN0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def err_rate(gt_s, s):\n",
        "\tc_x = best_map(gt_s,s)\n",
        "\terr_x = np.sum(gt_s[:] != c_x[:])\n",
        "\tmissrate = err_x.astype(float) / (gt_s.shape[0])\n",
        "\treturn missrate  \n",
        "\n",
        "data = sio.loadmat('COIL20.mat')\n",
        "Img = data['fea']\n",
        "Label = data['gnd']\n",
        "Img = np.reshape(Img,(Img.shape[0],32,32,1))\n",
        "\n",
        "n_input = [32,32]\n",
        "kernel_size = [3]\n",
        "n_hidden = [15]\n",
        "batch_size = 20*72\n",
        "model_path = '/home/pan/workspace-eclipse/deep-subspace-clustering/COIL20CodeModel/new/pretrain/model.ckpt'\n",
        "ft_path = '/home/pan/workspace-eclipse/deep-subspace-clustering/COIL20CodeModel/new/pretrain/model.ckpt'\n",
        "logs_path = '/home/pan/workspace-eclipse/deep-subspace-clustering/COIL20CodeModel/new/pretrain/logs'\n",
        "\n",
        "num_class = 20 #how many class we sample\n",
        "num_sa = 72\n",
        "\n",
        "batch_size_test = num_sa * num_class\n",
        "\n",
        "\n",
        "iter_ft = 0\n",
        "ft_times = 30\n",
        "display_step = ft_times\n",
        "alpha = 0.04\n",
        "learning_rate = 1e-3\n",
        "\n",
        "reg1 = 1.0\n",
        "reg2 = 150.0\n",
        "\n",
        "CAE = ConvAE(n_input = n_input, n_hidden = n_hidden, reg_const1 = reg1, reg_const2 = reg2, kernel_size = kernel_size, \\\n",
        "\t\t\tbatch_size = batch_size_test, model_path = model_path, logs_path= logs_path)\n",
        "\n",
        "acc_= []\n",
        "for i in range(0,1):\n",
        "\tcoil20_all_subjs = Img\n",
        "\tcoil20_all_subjs = coil20_all_subjs.astype(float)\t\n",
        "\tlabel_all_subjs = Label\n",
        "\tlabel_all_subjs = label_all_subjs - label_all_subjs.min() + 1    \n",
        "\tlabel_all_subjs = np.squeeze(label_all_subjs) \n",
        "\t \t\n",
        "\tCAE.initlization()\n",
        "\tCAE.restore()\n",
        "\t\n",
        "\tfor iter_ft  in range(ft_times):\n",
        "\t\titer_ft = iter_ft+1\n",
        "\t\tC,l1_cost,l2_cost = CAE.finetune_fit(coil20_all_subjs,learning_rate)\n",
        "\t\tif iter_ft % display_step == 0:\n",
        "\t\t\tprint (\"epoch: %.1d\" % iter_ft, \"cost: %.8f\" % (l1_cost/float(batch_size_test)))\n",
        "\t\t\tC = thrC(C,alpha)\n",
        "\t\t\t\n",
        "\t\t\ty_x, CKSym_x = post_proC(C, num_class, 12 ,8)\n",
        "\t\t\tmissrate_x = err_rate(label_all_subjs,y_x)\n",
        "\t\t\t\t\t\t\n",
        "\t\t\tacc = 1 - missrate_x\n",
        "\t\t\tprint (\"experiment: %d\" % i,\"acc: %.4f\" % acc)\n",
        "\tacc_.append(acc)\n",
        "\t\n",
        "acc_ = np.array(acc_)\n",
        "m = np.mean(acc_)\n",
        "me = np.median(acc_)\n",
        "print(m)\n",
        "print(me)\n",
        "print(acc_)\n",
        "\n",
        "\t\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JUHp1MvxNrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKU9YbCtxNp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}