{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep-NN_clustering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mxZOMIDXXau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import math\n",
        "import sys\n",
        "import logging\n",
        "#-----------------------------------------------------------\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from IPython.display import clear_output\n",
        "from scipy.spatial.distance import squareform, pdist\n",
        "from sklearn.preprocessing import normalize\n",
        "from numpy import linalg as LA\n",
        "from scipy.cluster.vq import kmeans, vq\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "from math import sqrt\n",
        "#----------------------------------------------\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from optparse import OptionParser\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYqX8ZR5wsGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def standardization(X):\n",
        "    return normalize(X, axis=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKho_sESzQMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def laplacian(A):\n",
        "    S = np.sum(A, 0)\n",
        "    D = np.diag(S)\n",
        "    D = LA.matrix_power(D, -1)\n",
        "    L = np.dot(D, A)\n",
        "    return L"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pUSpsQozS-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization(V):\n",
        "    return (V - min(V)) / (max(V) - min(V))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hndbmGyJzWIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Correlation_Similarity:\n",
        "    def get_matrix(self, Data):\n",
        "        X = standardization(Data)\n",
        "        X = pdist(X, 'correlation')\n",
        "        X = squareform(X)\n",
        "        L = laplacian(X)\n",
        "        Y = np.apply_along_axis(normalization, 1, L)\n",
        "        return Y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHrGqwRnzY-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Cosine_Similarity:\n",
        "    def get_matrix(self, Data):\n",
        "        X = standardization(Data)\n",
        "        X = pdist(X, 'cosine')\n",
        "        X = squareform(X)\n",
        "        L = laplacian(X)\n",
        "        Y = np.apply_along_axis(normalization, 1, L)\n",
        "        return Y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRV0iHGWzcp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Similarity_Dataset_Iterator():\n",
        "    def __init__(self, data, labels, similarity):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.matrix = similarity.get_matrix(data)\n",
        "        self.data_size = self.matrix.shape[0]\n",
        "        self.current_index = 0\n",
        "    def next_batch(self, num):\n",
        "        data=self.matrix.transpose()\n",
        "        labels=self.labels\n",
        "        idx = np.arange(0 , len(data))\n",
        "        np.random.shuffle(idx)\n",
        "        idx = idx[:num]\n",
        "        data_shuffle = [data[ i] for i in idx]\n",
        "        labels_shuffle = [labels[ i] for i in idx]\n",
        "        return data_shuffle, labels_shuffle\n",
        "    def whole_dataset(self):\n",
        "        return (self.matrix.transpose(), self.labels)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n076atQh0kmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5ed663d-3a76-4aa7-949e-03d98f6cb5b1"
      },
      "source": [
        "#from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3U9x9aT05Qz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "78874897-f51c-447c-b614-d0b4ddefca93"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/all_news.csv')\n",
        "df.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>topic</th>\n",
              "      <th>heading</th>\n",
              "      <th>content</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5f04d2481f35ed6864839349</td>\n",
              "      <td>[\"West-bengal\"]</td>\n",
              "      <td>[\"\\n                        \",\"\\n             ...</td>\n",
              "      <td>[\"Garbage-lined streets and overflowing drains...</td>\n",
              "      <td>[\"Garbage\",\"Salt-lake\",\"Bidhannagar-municipal-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5f04d24b1f35ed686483934a</td>\n",
              "      <td>[\"West-bengal\"]</td>\n",
              "      <td>[\"\\n                        \",\"\\n             ...</td>\n",
              "      <td>[\"The Bengal government will set up a plasma b...</td>\n",
              "      <td>[\"Calcutta-medical-college-and-hospital\",\"Coro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5f04d24d1f35ed686483934b</td>\n",
              "      <td>[\"West-bengal\"]</td>\n",
              "      <td>[\"\\n                        \",\"\\n             ...</td>\n",
              "      <td>[\"Bengal set another 24-hour record on Monday ...</td>\n",
              "      <td>[\"Lockdown\",\"Coronavirus\",\"Quarantine\"]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5f04d2501f35ed686483934c</td>\n",
              "      <td>[\"West-bengal\"]</td>\n",
              "      <td>[\"\\n                        \",\"\\n             ...</td>\n",
              "      <td>[\"Chief minister Mamata Banerjee on Monday sai...</td>\n",
              "      <td>[\"Mamata-banerjee\",\"Cyclone-amphan\"]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5f04d2531f35ed686483934d</td>\n",
              "      <td>[\"West-bengal\"]</td>\n",
              "      <td>[\"\\n                        \",\"\\n             ...</td>\n",
              "      <td>[\"Some senior historians have raised questions...</td>\n",
              "      <td>[\"Jagat-prakash-nadda\",\"Bharatiya-janata-party...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        _id  ...                                               tags\n",
              "0  5f04d2481f35ed6864839349  ...  [\"Garbage\",\"Salt-lake\",\"Bidhannagar-municipal-...\n",
              "1  5f04d24b1f35ed686483934a  ...  [\"Calcutta-medical-college-and-hospital\",\"Coro...\n",
              "2  5f04d24d1f35ed686483934b  ...            [\"Lockdown\",\"Coronavirus\",\"Quarantine\"]\n",
              "3  5f04d2501f35ed686483934c  ...               [\"Mamata-banerjee\",\"Cyclone-amphan\"]\n",
              "4  5f04d2531f35ed686483934d  ...  [\"Jagat-prakash-nadda\",\"Bharatiya-janata-party...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0GlS3WC1Udh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.dropna(subset=['content'], inplace = True)\n",
        "df.dropna(subset=['topic'], inplace = True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dat6uH3E2OmJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cced1ae-d391-4f6d-f054-a8e03a90c9ed"
      },
      "source": [
        "len(df.content.unique()),  len(df.content), len(df.topic)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1204, 1304, 1304)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S87soJgE2SZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "54489033-c90a-4c52-d068-6f818c65f370"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_id          0\n",
              "topic        0\n",
              "heading    383\n",
              "content      0\n",
              "tags        68\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PbPTbyu2deC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "340e2c3f-dfc3-4709-8bed-a12ddf583ab5"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1304, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-XKjCAS48pD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3ad50f97-3371-499a-d3ab-f8d4bcf72e1f"
      },
      "source": [
        "import re\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora,models\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "\n",
        "import nltk\n",
        "#nltk.download('wordnet')\n",
        "#nltk.download('stopwords')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2dezj2a48qS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = SnowballStemmer(language='english',ignore_stopwords=True)\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws57P9WE48uI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "3eb64829-6726-4133-b90f-7e0f43b0be4e"
      },
      "source": [
        "doc_sample = df['content'][0]\n",
        "print('original document: ')\n",
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print('\\n\\n tokenized and lemmatized document: ')\n",
        "print(preprocess(doc_sample))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original document: \n",
            "['[\"Garbage-lined', 'streets', 'and', 'overflowing', 'drains', 'across', 'Salt', 'Lake', 'have', 'sparked', 'fear', 'of', 'an', 'outbreak', 'of', 'enteric', 'diseases', 'and', 'are', 'forcing', 'residents', 'to', 'keep', 'the', 'windows', 'and', 'doors', 'of', 'their', 'houses', 'firmly', 'shut.\",\"Residents', 'said', 'most', 'of', 'the', '150', 'tonnes', 'of', 'the', 'garbage', 'Salt', 'Lake', 'generates', 'every', 'day', 'is', 'accumulating', 'across', 'the', '33.5sq', 'km', 'township.\",\"“At', 'this', 'rate,', 'Salt', 'Lake', 'will', 'soon', 'become', 'Dhapa.', 'This', 'is', 'happening', 'at', 'a', 'time', 'the', 'authorities', 'as', 'well', 'as', 'residents', 'need', 'to', 'focus', 'on', 'hygiene', 'to', 'combat', 'Covid', 'and', 'dengue,”', 'said', 'a', 'resident,', 'hand', 'firmly', 'on', 'his', 'nose.\",\"“It', 'is', 'true', 'that', 'garbage', 'has', 'accumulated', 'in', 'some', 'places', 'but', 'I', 'don’t', 'think', 'it', 'is', 'a', 'big', 'problem.', 'We', 'will', 'clear', 'it', 'within', 'seven', 'days,”', 'said', 'Debasish', 'Jana,', 'the', 'mayoral', 'council', 'member', 'in', 'charge', 'of', 'solid', 'waste', 'management', 'at', 'the', 'Bidhannagar', 'Municipal', 'Corporation.\",\"Asked', 'why', 'the', 'civic', 'body', 'has', 'not', 'yet', 'been', 'able', 'to', 'solve', 'a', '“simple', 'problem”,', 'Jana', 'said:', '“Several', 'of', 'our', 'trucks', 'are', 'out', 'of', 'order,', 'but', 'our', 'sweepers', 'are', 'at', 'work', 'all', 'day.', 'It’s', 'just', 'that', 'accumulated', 'garbage', 'isn’t', 'being', 'removed', 'regularly', 'from', 'a', 'few', 'areas.”\",\"For', 'five', 'days', 'a', 'week,', 'Buddhadeb', 'Basu,', 'a', 'resident', 'of', 'DB', 'block,', 'near', 'City', 'Centre,', 'is', 'having', 'to', 'dump', 'household', 'waste', 'at', 'the', 'neighbourhood', 'compactor', 'station', 'overflowing', 'with', 'garbage.', 'He', 'doesn’t', 'have', 'a', 'choice.', 'His', 'neighbours,', 'too,', 'are', 'doing', 'the', 'same.\",\"“Civic', 'workers', 'would', 'previously', 'collect', 'garbage', 'from', 'our', 'doorstep', 'every', 'day.', 'Now', 'they', 'come', 'hardly', 'twice', 'a', 'week.', 'They', 'are', 'not', 'even', 'taking', 'away', 'garbage', 'from', 'near', 'the', 'compactor', 'station.', 'The', 'entire', 'area', 'stinks,”', 'the', '62-year-old', 'told', '\",\"The', 'Telegraph\",\".\",\"This', 'newspaper', 'drove', 'around', 'the', 'township', 'on', 'Monday', 'and', 'found', 'scarcely', 'any', 'block', 'or', 'compactor', 'station', 'that', 'did', 'not', 'have', 'a', 'heap', 'of', 'garbage', 'lying', 'around.\",\"The', 'areas', 'near', 'City', 'Centre', 'and', 'the', 'CGO', 'Complex', 'looked', 'the', 'dirtiest.', 'The', 'situation', 'was', 'similar', 'in', 'the', 'AE,', 'AB,', 'BC,', 'BB,', 'and', 'IC', 'blocks.\",\"A', '20-metre', 'stretch', 'from', 'the', 'City', 'Centre', 'Metro', 'station', 'till', 'Bidhannagar', 'College', 'had', 'almost', 'half', 'the', 'carriageway', 'swallowed', 'by', 'garbage.', 'The', 'entire', 'area', 'had', 'flies', 'swarming', 'around,', 'while', 'several', 'dogs', 'and', 'cows', 'were', 'seen', 'rummaging', 'through', 'the', 'garbage', 'for', 'food.\",\"Passers-by', 'were', 'seen', 'holding', 'handkerchiefs', 'on', 'their', 'noses', 'despite', 'wearing', 'masks.\",\"“The', 'stench', 'from', 'the', 'garbage', 'in', 'front', 'of', 'our', 'house', 'is', 'so', 'overpowering', 'that', 'we', 'keep', 'the', 'windows', 'of', 'our', 'house', 'shut,”', 'said', 'Prakash', 'Agarwal,', 'a', 'resident', 'of', 'EC', 'Block.\",\"A', 'civic', 'official', 'blamed', 'the', 'combination', 'of', 'staff', 'shortage', 'and', 'defunct', 'garbage', 'collection', 'trucks', 'for', 'the', 'sorry', 'state', 'of', 'affairs.\\xa0', '“Many', 'men', 'are', 'not', 'reporting', 'for', 'work', 'out', 'of', 'fear', 'of', 'contracting', 'the', 'coronavirus', 'and', 'also', 'because', 'local', 'trains', 'are', 'not', 'running,”', 'the', 'official', 'said.', '“There', 'used', 'to', 'be', '250', 'sweepers.', 'Now,', 'there', 'are', 'only', 'half', 'that\\xa0number.”\",\"Mayoral', 'council', 'member', 'Jana', 'said', 'many', 'workers', 'who', 'were', 'involved', 'in', 'collection', 'of', 'garbage', 'were', 'now', 'selling', 'vegetables', 'or', 'involved', 'in', 'other', 'activities.', '“We', 'are', 'trying', 'to', 'hire', 'personnel', 'but', 'the', 'pandemic', 'is', 'coming', 'in', 'the', 'way,”', 'he', 'said.\",\"To', 'add', 'to', 'the', 'problem,', 'sources', 'said,', 'many', 'handcarts', 'and', 'vans', 'have', 'broken', 'down.\"]']\n",
            "\n",
            "\n",
            " tokenized and lemmatized document: \n",
            "['garbag', 'line', 'street', 'overflow', 'drain', 'salt', 'lake', 'spark', 'fear', 'outbreak', 'enter', 'diseas', 'forc', 'resid', 'window', 'door', 'hous', 'firm', 'shut', 'resid', 'say', 'tonn', 'garbag', 'salt', 'lake', 'generat', 'accumul', 'township', 'rate', 'salt', 'lake', 'soon', 'dhapa', 'happen', 'time', 'author', 'resid', 'need', 'focus', 'hygien', 'combat', 'covid', 'dengu', 'say', 'resid', 'hand', 'firm', 'nose', 'true', 'garbag', 'accumul', 'place', 'think', 'problem', 'clear', 'seven', 'day', 'say', 'debasish', 'jana', 'mayor', 'council', 'member', 'charg', 'solid', 'wast', 'manag', 'bidhannagar', 'municip', 'corpor', 'ask', 'civic', 'bodi', 'abl', 'solv', 'simpl', 'problem', 'jana', 'say', 'truck', 'order', 'sweeper', 'work', 'accumul', 'garbag', 'remov', 'regular', 'area', 'day', 'week', 'buddhadeb', 'basu', 'resid', 'block', 'near', 'citi', 'centr', 'have', 'dump', 'household', 'wast', 'neighbourhood', 'compactor', 'station', 'overflow', 'garbag', 'choic', 'neighbour', 'civic', 'worker', 'previous', 'collect', 'garbag', 'doorstep', 'come', 'hard', 'twice', 'week', 'take', 'away', 'garbag', 'near', 'compactor', 'station', 'entir', 'area', 'stink', 'year', 'tell', 'telegraph', 'newspap', 'drive', 'township', 'monday', 'scarc', 'block', 'compactor', 'station', 'heap', 'garbag', 'lie', 'area', 'near', 'citi', 'centr', 'complex', 'look', 'dirtiest', 'situat', 'similar', 'block', 'metr', 'stretch', 'citi', 'centr', 'metro', 'station', 'till', 'bidhannagar', 'colleg', 'half', 'carriageway', 'swallow', 'garbag', 'entir', 'area', 'fli', 'swarm', 'dog', 'cow', 'see', 'rummag', 'garbag', 'food', 'passer', 'see', 'hold', 'handkerchief', 'nose', 'despit', 'wear', 'mask', 'stench', 'garbag', 'hous', 'overpow', 'window', 'hous', 'shut', 'say', 'prakash', 'agarw', 'resid', 'block', 'civic', 'offici', 'blame', 'combin', 'staff', 'shortag', 'defunct', 'garbag', 'collect', 'truck', 'sorri', 'state', 'affair', 'report', 'work', 'fear', 'contract', 'coronavirus', 'local', 'train', 'run', 'offici', 'say', 'sweeper', 'half', 'number', 'mayor', 'council', 'member', 'jana', 'say', 'worker', 'involv', 'collect', 'garbag', 'sell', 'veget', 'involv', 'activ', 'tri', 'hire', 'personnel', 'pandem', 'come', 'say', 'problem', 'sourc', 'say', 'handcart', 'van', 'break']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzga2tYu48mP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "e1bc92a9-01a3-4378-c4b4-4b6fa441bcc9"
      },
      "source": [
        "#df['processed_heading'] = df['heading'].map(preprocess)\n",
        "df['processed_content'] = df['content'].map(preprocess)\n",
        "df['processed_topic'] = df['topic'].map(preprocess)\n",
        "#df['processed_heading'] = df['processed_heading'].apply(lambda x: ' '.join(x))\n",
        "df['processed_content'] = df['processed_content'].apply(lambda x: ' '.join(x))\n",
        "df['processed_topic'] = df['processed_topic'].apply(lambda x: ' '.join(x))\n",
        "df.head()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>topic</th>\n",
              "      <th>heading</th>\n",
              "      <th>content</th>\n",
              "      <th>tags</th>\n",
              "      <th>processed_content</th>\n",
              "      <th>processed_topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5f04d2481f35ed6864839349</td>\n",
              "      <td>[\"West-bengal\"]</td>\n",
              "      <td>[\"\\n                        \",\"\\n             ...</td>\n",
              "      <td>[\"Garbage-lined streets and overflowing drains...</td>\n",
              "      <td>[\"Garbage\",\"Salt-lake\",\"Bidhannagar-municipal-...</td>\n",
              "      <td>garbag line street overflow drain salt lake sp...</td>\n",
              "      <td>west bengal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5f04d24b1f35ed686483934a</td>\n",
              "      <td>[\"West-bengal\"]</td>\n",
              "      <td>[\"\\n                        \",\"\\n             ...</td>\n",
              "      <td>[\"The Bengal government will set up a plasma b...</td>\n",
              "      <td>[\"Calcutta-medical-college-and-hospital\",\"Coro...</td>\n",
              "      <td>bengal govern plasma bank calcutta medic colle...</td>\n",
              "      <td>west bengal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5f04d24d1f35ed686483934b</td>\n",
              "      <td>[\"West-bengal\"]</td>\n",
              "      <td>[\"\\n                        \",\"\\n             ...</td>\n",
              "      <td>[\"Bengal set another 24-hour record on Monday ...</td>\n",
              "      <td>[\"Lockdown\",\"Coronavirus\",\"Quarantine\"]</td>\n",
              "      <td>bengal hour record monday highest number covid...</td>\n",
              "      <td>west bengal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5f04d2501f35ed686483934c</td>\n",
              "      <td>[\"West-bengal\"]</td>\n",
              "      <td>[\"\\n                        \",\"\\n             ...</td>\n",
              "      <td>[\"Chief minister Mamata Banerjee on Monday sai...</td>\n",
              "      <td>[\"Mamata-banerjee\",\"Cyclone-amphan\"]</td>\n",
              "      <td>chief minist mamata banerje monday say problem...</td>\n",
              "      <td>west bengal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5f04d2531f35ed686483934d</td>\n",
              "      <td>[\"West-bengal\"]</td>\n",
              "      <td>[\"\\n                        \",\"\\n             ...</td>\n",
              "      <td>[\"Some senior historians have raised questions...</td>\n",
              "      <td>[\"Jagat-prakash-nadda\",\"Bharatiya-janata-party...</td>\n",
              "      <td>senior historian rais question attempt pitch b...</td>\n",
              "      <td>west bengal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        _id  ... processed_topic\n",
              "0  5f04d2481f35ed6864839349  ...     west bengal\n",
              "1  5f04d24b1f35ed686483934a  ...     west bengal\n",
              "2  5f04d24d1f35ed686483934b  ...     west bengal\n",
              "3  5f04d2501f35ed686483934c  ...     west bengal\n",
              "4  5f04d2531f35ed686483934d  ...     west bengal\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfAJav1NLOVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('/content/drive/My Drive/edited_all_news.csv')"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsa-O7Bw48kz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "18b37dce-37f3-422e-ef4c-b1aac430c293"
      },
      "source": [
        "from collections import Counter\n",
        "Counter(df.topic[700:])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'[\"Business\"]': 57,\n",
              "         '[\"City\"]': 22,\n",
              "         '[\"DHNS\"]': 1,\n",
              "         '[\"Entertainment\"]': 34,\n",
              "         '[\"International\"]': 27,\n",
              "         '[\"Lifestyle\"]': 36,\n",
              "         '[\"Metrolife\"]': 12,\n",
              "         '[\"Nation\"]': 109,\n",
              "         '[\"National\"]': 62,\n",
              "         '[\"Opinion\"]': 58,\n",
              "         '[\"People\"]': 1,\n",
              "         '[\"Science and Environment\"]': 17,\n",
              "         '[\"Science\"]': 3,\n",
              "         '[\"Specials\"]': 15,\n",
              "         '[\"Spectrum\"]': 2,\n",
              "         '[\"Sports\"]': 44,\n",
              "         '[\"State\"]': 31,\n",
              "         '[\"Sunday Chronicle\"]': 2,\n",
              "         '[\"Sunday Herald\"]': 1,\n",
              "         '[\"Supplements\"]': 5,\n",
              "         '[\"Technology\"]': 23,\n",
              "         '[\"World\"]': 42})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQBuLrsC3FWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_NewsGroup_data(similarity):    \n",
        "    logging.basicConfig(level=logging.INFO,\n",
        "                        format='%(asctime)s %(levelname)s %(message)s')\n",
        "    op = OptionParser()\n",
        "    op.add_option(\"--lsa\", dest=\"n_components\", type=\"int\",\n",
        "                  help=\"Preprocess documents with latent semantic analysis.\")    \n",
        "    op.add_option(\"--no-idf\",action=\"store_false\", dest=\"use_idf\", default=True,\n",
        "                  help=\"Disable Inverse Document Frequency feature weighting.\")\n",
        "    op.add_option(\"--use-hashing\", action=\"store_true\", default=False,\n",
        "                  help=\"Use a hashing feature vectorizer\")\n",
        "    op.add_option(\"--n-features\", type=int, default=10000,\n",
        "                  help=\"Maximum number of features to extract from text.\")    \n",
        "    def is_interactive():\n",
        "        return not hasattr(sys.modules['__main__'], '__file__')\n",
        "    argv = [] if is_interactive() else sys.argv[1:]\n",
        "    (opts, args) = op.parse_args(argv)\n",
        "    if len(args) > 0:\n",
        "        op.error(\"this script takes no arguments.\")\n",
        "        sys.exit(1)\n",
        "        \n",
        "    labels = df.processed_topic[700:]\n",
        "    #true_k = np.unique(labels).shape[0]\n",
        "    vectorizer = TfidfVectorizer(max_features=opts.n_features,use_idf=opts.use_idf)\n",
        "    X = vectorizer.fit_transform(df.processed_content[700:])\n",
        "    if opts.n_components:\n",
        "        svd = TruncatedSVD(opts.n_components)\n",
        "        normalizer = Normalizer(copy=False)\n",
        "        lsa = make_pipeline(svd, normalizer)\n",
        "        X = lsa.fit_transform(X)\n",
        "        explained_variance = svd.explained_variance_ratio_.sum()\n",
        "    return Similarity_Dataset_Iterator(X.toarray(), labels, similarity)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZHYkqiV6Gsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call Correlation_Similarity as similarity dataset.\n",
        "trainSet_correlation = read_NewsGroup_data(Correlation_Similarity())"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZQ9wy646MCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call Cosine_Similarity as similarity dataset.\n",
        "trainSet_cosine = read_NewsGroup_data(Cosine_Similarity())"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIbetWyC6PMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbee3907-c178-42ff-9177-7961f78568bc"
      },
      "source": [
        "n_input = trainSet_correlation.data_size #--------- Number of input data.\n",
        "print(n_input)\n",
        "# Define the number of hidden layer. \n",
        "if n_input >= 1024:\n",
        "    Nn = int(2048)\n",
        "elif n_input >= 512:\n",
        "    Nn = int(1024)\n",
        "elif n_input >= 256:\n",
        "    Nn = int(512)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN3m9vsd6X0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_hidden_1 = int(Nn/2) #-------------------- The autoencoder hidden layer 1.\n",
        "n_code = str(int(n_hidden_1/2)) #----------- The number of output dimension value."
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DeA7y-46cOk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "cb1aac2e-c96b-4c90-f05a-ca4f78f7c667"
      },
      "source": [
        "print('Layer 1: -----------', n_input)\n",
        "print('Layer 2: -----------', n_hidden_1)\n",
        "print('Layer 3: -----------', int(n_code))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer 1: ----------- 604\n",
            "Layer 2: ----------- 512\n",
            "Layer 3: ----------- 256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0McRy7WF6gX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_means_(X, n_clusters):\n",
        "    kmeans_centroids,_ =  kmeans(X, n_clusters)\n",
        "    kmeans_, _ = vq(X, kmeans_centroids)\n",
        "    return kmeans_"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIQwaaHt6nG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder(x, n_code, mode_train):    \n",
        "    with tf.variable_scope(\"encoder\"):        \n",
        "        with tf.variable_scope(\"hidden-layer-1\"):\n",
        "            hidden_1 = layer(x, [n_input, n_hidden_1], [n_hidden_1], mode_train)\n",
        "        with tf.variable_scope(\"embedded\"):\n",
        "            code = layer(hidden_1, [n_hidden_1, n_code], [n_code], mode_train)\n",
        "    return code"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu9rxQoz6p6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder(code, n_code, mode_train):\n",
        "    with tf.variable_scope(\"decoder\"):\n",
        "        with tf.variable_scope(\"hidden-layer-1\"):\n",
        "            hidden_1 = layer(code, [n_code, n_hidden_1], [n_hidden_1], mode_train)\n",
        "        with tf.variable_scope(\"reconstructed\"):\n",
        "            output = layer(hidden_1, [n_hidden_1, n_input], [n_input], mode_train)\n",
        "    return output"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8C3fm4r6tdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_norm(x, n_out, mode_train):\n",
        "    beta_initialize = tf.constant_initializer(value=0.1, dtype=tf.float32)\n",
        "    gamma_initialize = tf.constant_initializer(value=0.1, dtype=tf.float32)\n",
        "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_initialize)\n",
        "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_initialize)\n",
        "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
        "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
        "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
        "    def mean_var():\n",
        "        with tf.control_dependencies([ema_apply_op]):\n",
        "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
        "    mean, var = control_flow_ops.cond(mode_train, mean_var, lambda: (ema_mean, ema_var))\n",
        "    reshaped_x = tf.reshape(x, [-1, 1, 1, n_out])\n",
        "    normed = tf.nn.batch_norm_with_global_normalization(reshaped_x, mean, var, beta, gamma, 1e-08, True)\n",
        "    return tf.reshape(normed, [-1, n_out])"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPomeQMg61f5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def layer(input, weight_shape, bias_shape, mode_train):\n",
        "    value_initialize = (1.0 / weight_shape[0] ** 0.5)\n",
        "    weight_initialize = tf.random_normal_initializer(stddev = value_initialize, seed = None)\n",
        "    bias_initialize = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
        "    w = tf.get_variable(\"w\", weight_shape, initializer=weight_initialize)\n",
        "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_initialize)\n",
        "    return tf.nn.sigmoid(batch_norm((tf.matmul(input, w) + b), weight_shape[1], mode_train))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuq5HnwW642G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(reconstructed, x):\n",
        "    with tf.variable_scope(\"train\"):\n",
        "        train_loss = tf.reduce_mean(tf.reduce_sum(tf.square(tf.subtract(reconstructed, x)), 1))\n",
        "        return train_loss"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbAIvq9769F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(cost, learning_rate, beta1, beta2, global_step):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon=1e-08, use_locking=False, name='Adam')\n",
        "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
        "    return train_op"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gIEYZXv7AUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bac3d1aa-67bf-4e15-da51-33d14d2db314"
      },
      "source": [
        "# Parameters\n",
        "n_layers = 3 #------------------------------ Number of Neural Networks Layers.\n",
        "beta1 = 0.9 #------------------------------- The decay rate 1.  \n",
        "beta2 = 0.999 #----------------------------- The decay rate 2.\n",
        "learning_rate = (beta1/n_input) #----------- The learning rate.\n",
        "n_batch = math.ceil(sqrt(sqrt(n_input))) #-- Number of selection data in per step.\n",
        "n_backpro = math.ceil(n_input/n_batch) #---- Number of Backpro in per epoch.\n",
        "n_clusters = 3 #---------------------------- Number of clusters.\n",
        "\n",
        "print(n_batch)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPCpkiff7Kxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_cor, labels_cor = trainSet_correlation.whole_dataset() #-- Allocation of data and labels\n",
        "data_cos, labels_cos = trainSet_cosine.whole_dataset() #------- Allocation of data and labels"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M5XoVMB7glv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_cor=[] #--------------------------- A list to keep all NMI scores.\n",
        "loss_cost_cor=[] #------------------------- A list to keep all training evaluations.\n",
        "seeding_cor=[] #--------------------------- A list to keep all steps."
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzhSKWs97k7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "521ea0e1-68f7-4c14-935c-78648532cf16"
      },
      "source": [
        "for i in range(1, 11):\n",
        "    with tf.Graph().as_default():    \n",
        "        with tf.variable_scope(\"autoencoder_architecture\"):\n",
        "            x = tf.placeholder(\"float\", [None, n_input])   \n",
        "            mode_train = tf.placeholder(tf.bool)\n",
        "            code = encoder(x, int(n_code), mode_train)\n",
        "            reconstructed = decoder(code, int(n_code), mode_train)\n",
        "            cost = loss(reconstructed, x)\n",
        "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "            train_optimizer = training(cost, learning_rate, beta1, beta2, global_step)\n",
        "            sess = tf.Session()\n",
        "            init_op = tf.global_variables_initializer()\n",
        "            sess.run(init_op)\n",
        "            # Training cycle\n",
        "            for ii in range(n_layers):\n",
        "                # Fit training with backpropagation using batch data.\n",
        "                for jj in range(n_backpro):\n",
        "                    miniData, _ = trainSet_correlation.next_batch(n_batch)\n",
        "                    _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n",
        "                                                                              mode_train: True})       \n",
        "                #------------------------- End of the Optimization ------------------------------\n",
        "                \n",
        "    # Getting embedded codes and running K-Means on them.\n",
        "    ae_codes_cor = sess.run(code, feed_dict={x: data_cor, mode_train: False})        \n",
        "    idx_cor = k_means_(ae_codes_cor, n_clusters)\n",
        "    ae_nmi_cor = normalized_mutual_info_score(labels_cor, idx_cor)\n",
        "    ae_nmi_cor = ae_nmi_cor*100\n",
        "    results_cor.append(ae_nmi_cor)    \n",
        "    seeding_cor.append(i)\n",
        "    loss_cost_cor.append(new_cost)    \n",
        "    print(\"NMI score for AE is: {:0.2f} and new cost is: {:0.2f} in {:d} step of seeding.\"\n",
        "          .format(ae_nmi_cor, new_cost, i))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-58355562b610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;31m# Fit training with backpropagation using batch data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_backpro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0mminiData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainSet_correlation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n\u001b[1;32m     20\u001b[0m                                                                               mode_train: True})       \n",
            "\u001b[0;32m<ipython-input-8-c573112324ab>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdata_shuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlabels_shuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_shuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwhole_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c573112324ab>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdata_shuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlabels_shuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_shuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwhole_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4403\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4404\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4405\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4406\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 88"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ulspmFn7tu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The Average of NMI Score for >>> {:d} <<< Random Factors in Autoencoder Correlation is >>> {:0.2f} <<<\"\n",
        "      .format(len(seeding_cor), (np.mean(results_cor))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND1MKJ6iE8Ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_cor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXecHj3NE8Tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_cos=[] #--------------------------- A list to keep all NMI scores.\n",
        "loss_cost_cos=[] #------------------------- A list to keep all training evaluations.\n",
        "seeding_cos=[] #--------------------------- A list to keep all steps."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TERMUm14E8SQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1, 11):\n",
        "    with tf.Graph().as_default():    \n",
        "        with tf.variable_scope(\"autoencoder_architecture\"):\n",
        "            x = tf.placeholder(\"float\", [None, n_input])   \n",
        "            mode_train = tf.placeholder(tf.bool)\n",
        "            code = encoder(x, int(n_code), mode_train)\n",
        "            reconstructed = decoder(code, int(n_code), mode_train)\n",
        "            cost = loss(reconstructed, x)\n",
        "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "            train_optimizer = training(cost, learning_rate, beta1, beta2, global_step)\n",
        "            sess = tf.Session()\n",
        "            init_op = tf.global_variables_initializer()\n",
        "            sess.run(init_op)\n",
        "            # Training cycle\n",
        "            for ii in range(n_layers):\n",
        "                # Fit training with backpropagation using batch data.\n",
        "                for jj in range(n_backpro):\n",
        "                    miniData, _ = trainSet_cosine.next_batch(n_batch)\n",
        "                    _, new_cost = sess.run([train_optimizer,cost], feed_dict={x: miniData,\n",
        "                                                                              mode_train: True})       \n",
        "                #------------------------- End of the Optimization ------------------------------\n",
        "\n",
        "    # Getting embedded codes and running K-Means on them.\n",
        "    ae_codes_cos = sess.run(code, feed_dict={x: data_cos, mode_train: False})        \n",
        "    idx_cos = k_means_(ae_codes_cos, n_clusters)\n",
        "    ae_nmi_cos = normalized_mutual_info_score(labels_cos, idx_cos)\n",
        "    ae_nmi_cos = ae_nmi_cos*100\n",
        "    results_cos.append(ae_nmi_cos)    \n",
        "    seeding_cos.append(i)\n",
        "    loss_cost_cos.append(new_cost)    \n",
        "    print(\"NMI score for AE is: {:0.2f} and new cost is: {:0.2f} in {:d} step of seeding.\"\n",
        "          .format(ae_nmi_cos, new_cost, i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL6sjMQEFIXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The Average of NMI Score for >>> {:d} <<< Random Factors in Autoencoder Cosine is >>> {:0.2f} <<<\"\n",
        "      .format(len(seeding_cos), (np.mean(results_cos))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRQmt5pjFIa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_cos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3afCnSOFIVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "plt.ylim(30,101)\n",
        "plt.plot(seeding_cor, results_cor, label='Autoencoder Correlation Simialrity', color='m', marker='o')\n",
        "plt.plot(seeding_cos, results_cos, label='Autoencoder Cosine Simialrity', color='g', marker='s')\n",
        "plt.xlabel('Number of Seeding.')\n",
        "plt.ylabel('NMI')\n",
        "plt.grid()\n",
        "plt.title('The Average of NMI Scores')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIhu5zg4FeA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Autoencoder Clustering on Cosine: ------------ {:0.2f}\".format(np.mean(results_cos)))\n",
        "print(\"Autoencoder Clustering on Correlation: ------- {:0.2f}\".format(np.mean(results_cor)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}